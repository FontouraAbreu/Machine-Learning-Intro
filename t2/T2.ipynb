{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística\n",
    "\n",
    "Neste trabalho, você implementará a regressão logística e a aplicará a dois conjuntos de dados diferentes.\n",
    "\n",
    "# Sumário\n",
    "- [ 1 - Pacotes ](#1)\n",
    "- [ 2 - Regressão Logística](#2)\n",
    "  - [ 2.1 Declaração do Problema](#2.1)\n",
    "  - [ 2.2 Carregamento e visualização dos dados](#2.2)\n",
    "  - [ 2.3 Função Sigmoide](#2.3)\n",
    "  - [ 2.4 Função de Custo para Regressão Logística](#2.4)\n",
    "  - [ 2.5 Gradiente para Regressão Logística](#2.5)\n",
    "  - [ 2.6 Aprendizagem de parâmetros usando o gradiente descendente ](#2.6)\n",
    "  - [ 2.7 Plotagem da Fronteira de Decisão](#2.7)\n",
    "  - [ 2.8 Avaliação da Regressão Logística](#2.8)\n",
    "- [ 3 - Regressão Logística Regularizada](#3)\n",
    "  - [ 3.1 Declaração do Problema](#3.1)\n",
    "  - [ 3.2 Carregamento e visualização dos dados](#3.2)\n",
    "  - [ 3.3 Mapeamento de Features](#3.3)\n",
    "  - [ 3.4 Função de Custo para Regressão Logística Regularizada](#3.4)\n",
    "  - [ 3.5 Gradiente para Regressão Logística Regularizada](#3.5)\n",
    "  - [ 3.6 Aprendizagem de parâmetros usando o gradiente descendente](#3.6)\n",
    "  - [ 3.7 Plotagem da Fronteira de Decisão](#3.7)\n",
    "  - [ 3.8 Avaliação do modelo de Regressão Logística Regularizada](#3.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** Para evitar erros do corretor automático, é apenas permitido editar os código com o comentário no início `# GRADED FUNCTION` (que são, efetivamente, os 6 códigos que você deve preencher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Pacotes \n",
    "\n",
    "Primeiro, vamos executar a célula abaixo para importar todos os pacotes de que você precisará durante esta tarefa.\n",
    "- [numpy](www.numpy.org) é o pacote fundamental para computação científica com Python.\n",
    "- [matplotlib](http://matplotlib.org) é uma biblioteca famosa para plotar gráficos em Python.\n",
    "-  ``utils.py`` contém funções auxiliares para esta tarefa. Você não precisa modificar o código neste arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import copy\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Regressão Logística\n",
    "\n",
    "Nesta parte do exercício, você construirá um modelo de regressão logística para prever se um estudante será admitido numa universidade.\n",
    "\n",
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Declaração do Problema\n",
    "\n",
    "Suponha que você seja o administrador de um departamento universitário e queira determinar a chance de admissão de cada candidato com base nos seus resultados em dois exames. \n",
    "* Você tem dados históricos de candidatos anteriores que pode usar como um conjunto de treinamento para regressão logística. \n",
    "* Para cada exemplo de treinamento, você tem as notas do candidato em dois exames e a decisão de admissão. \n",
    "* Sua tarefa é construir um modelo de classificação que estime a probabilidade de admissão de um candidato com base nas notas desses dois exames. \n",
    "\n",
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Carregamento e visualização dos dados\n",
    "\n",
    "Você começará carregando o conjunto de dados para esta tarefa. \n",
    "- A função `load_data()` mostrada abaixo carrega os dados nas variáveis `X_train` e `y_train`\n",
    "  - `X_train` contém as notas em dois exames para um estudante\n",
    "  - `y_train` é a decisão de admissão \n",
    "      - `y_train = 1` se o estudante foi admitido \n",
    "      - `y_train = 0` se o estudante não foi admitido \n",
    "  - Tanto `X_train` quanto `y_train` são arrays numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# carregar conjunto de dados\n",
    "X_train, y_train = load_data(\"data/ex2data1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar as variáveis\n",
    "Vamos nos familiarizar mais com o seu conjunto de dados.  \n",
    "- Um bom ponto de partida é simplesmente imprimir cada variável e ver o que ela contém.\n",
    "\n",
    "O código abaixo imprime os primeiros cinco valores de `X_train` e o tipo da variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os primeiros cinco elementos em X_train são:\n",
      " [[34.62365962 78.02469282]\n",
      " [30.28671077 43.89499752]\n",
      " [35.84740877 72.90219803]\n",
      " [60.18259939 86.3085521 ]\n",
      " [79.03273605 75.34437644]]\n",
      "Tipo de X_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Os primeiros cinco elementos em X_train são:\\n\", X_train[:5])\n",
    "print(\"Tipo de X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora imprima os primeiros cinco valores de `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os primeiros cinco elementos em y_train são:\n",
      " [0. 0. 0. 1. 1.]\n",
      "Tipo de y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Os primeiros cinco elementos em y_train são:\\n\", y_train[:5])\n",
    "print(\"Tipo de y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificar as dimensões das suas variáveis\n",
    "\n",
    "Outra forma útil de se familiarizar com seus dados é visualizar suas dimensões. Vamos imprimir o formato de `X_train` e `y_train` e ver quantos exemplos de treinamento temos em nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O formato de X_train é: (100, 2)\n",
      "O formato de y_train é: (100,)\n",
      "Temos m = 100 exemplos de treinamento\n"
     ]
    }
   ],
   "source": [
    "print ('O formato de X_train é: ' + str(X_train.shape))\n",
    "print ('O formato de y_train é: ' + str(y_train.shape))\n",
    "print ('Temos m = %d exemplos de treinamento' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar seus dados\n",
    "\n",
    "Antes de começar a implementar qualquer algoritmo de aprendizagem, é sempre bom visualizar os dados, se possível.\n",
    "- O código abaixo exibe os dados num gráfico 2D (como mostrado abaixo), onde os eixos são as notas dos dois exames, e os exemplos positivos e negativos são mostrados com marcadores diferentes.\n",
    "- Usamos uma função auxiliar no arquivo ``utils.py`` para gerar este gráfico. \n",
    "\n",
    "<img src=\"images/figure 1.png\" width=\"450\" height=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotar exemplos\n",
    "plot_data(X_train, y_train[:], pos_label=\"Admitido\", neg_label=\"Não admitido\")\n",
    "\n",
    "# Definir o rótulo do eixo y\n",
    "plt.ylabel('Nota do Exame 2') \n",
    "# Definir o rótulo do eixo x\n",
    "plt.xlabel('Nota do Exame 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seu objetivo é construir um modelo de regressão logística para ajustar estes dados.\n",
    "- Com este modelo, você pode então prever se um novo estudante será admitido com base nas suas notas nos dois exames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.3\"></a>\n",
    "### 2.3 Função Sigmoide\n",
    "\n",
    "Lembre-se de que para a regressão logística, o modelo é representado como\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "onde a função $g$ é a função sigmoide. A função sigmoide é definida como:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Vamos implementar a função sigmoide primeiro, para que possa ser usada no resto desta tarefa.\n",
    "\n",
    "<a name='ex-01'></a>\n",
    "### Exercício 1\n",
    "Por favor, complete a função `sigmoid` para calcular\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Note que \n",
    "- `z` nem sempre é um único número, mas também pode ser um array de números. \n",
    "- Se a entrada for um array de números, gostaríamos de aplicar a função sigmoide a cada valor no array de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcula o sigmoide de z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): Um escalar, array numpy de qualquer tamanho.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), com o mesmo formato que z\n",
    "         \n",
    "    \"\"\"\n",
    "          \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    g = 1 / (1 + np.exp(-z)) \n",
    "    \n",
    "    ### END SOLUTION ###  \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando terminar, tente testar alguns valores chamando `sigmoid(x)` na célula abaixo. \n",
    "- Para valores grandes e positivos de x, o sigmoide deve estar perto de 1, enquanto para valores grandes e negativos, o sigmoide deve estar perto de 0. \n",
    "- Avaliar `sigmoid(0)` deve dar exatamente 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Nota: Você pode editar este valor\n",
    "value = 0\n",
    "\n",
    "print (f\"sigmoid({value}) = {sigmoid(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>sigmoid(0)<b></td>\n",
    "    <td> 0.5 </td> \n",
    "  </tr>\n",
    "</table>\n",
    "    \n",
    "- Como mencionado antes, seu código também deve funcionar com vetores e matrizes. Para uma matriz, sua função deve executar a função sigmoide em cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([ -1, 0, 1, 2]) = [0.26894142 0.5        0.73105858 0.88079708]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([ -1, 0, 1, 2]) = \" + str(sigmoid(np.array([-1, 0, 1, 2]))))\n",
    "\n",
    "# UNIT TESTS\n",
    "from public_tests import *\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b>sigmoid([-1, 0, 1, 2])<b></td> \n",
    "    <td>[0.26894142        0.5           0.73105858        0.88079708]</td> \n",
    "  </tr>    \n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.4\"></a>\n",
    "### 2.4 Função de Custo para Regressão Logística\n",
    "\n",
    "Nesta seção, você implementará a função de custo para regressão logística.\n",
    "\n",
    "<a name='ex-02'></a>\n",
    "### Exercício 2\n",
    "\n",
    "Por favor, complete a função `compute_cost` usando as equações abaixo.\n",
    "\n",
    "Lembre-se que para a regressão logística, a função de custo tem a forma \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "onde\n",
    "* m é o número de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ é o custo para um único ponto de dados, que é - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ é a previsão do modelo, enquanto $y^{(i)}$, que é o rótulo real\n",
    "\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ onde a função $g$ é a função sigmoide.\n",
    "    * Pode ser útil calcular primeiro uma variável intermediária $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ onde $n$ é o número de features, antes de calcular $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$\n",
    "\n",
    "Nota:\n",
    "* Ao fazer isso, lembre-se de que as variáveis `X_train` e `y_train` não são valores escalares, mas sim matrizes de formato ($m, n$) e ($𝑚$,1) respetivamente, onde $𝑛$ é o número de features e $𝑚$ é o número de exemplos de treinamento.\n",
    "* Você pode usar a função sigmoide que implementou acima para esta parte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_cost\n",
    "def compute_cost(X, y, w, b, *argv):\n",
    "    \"\"\"\n",
    "    Calcula o custo sobre todos os exemplos\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,))  valor alvo \n",
    "      w : (ndarray Shape (n,))  valores dos parâmetros do modelo      \n",
    "      b : (scalar)              valor do parâmetro de bias do modelo\n",
    "      *argv : não usado, para compatibilidade com a versão regularizada abaixo\n",
    "    Returns:\n",
    "      total_cost : (scalar) custo \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><font size=\"3\" color=\"darkgreen\"><b>Clique para ver as dicas</b></font></summary>\n",
    "    \n",
    "* Você pode representar um operador de somatório eg: $h = \\sum\\limits_{i = 0}^{m-1} 2i$ em código da seguinte forma:\n",
    "\n",
    "```python\n",
    "    h = 0\n",
    "    for i in range(m):\n",
    "        h = h + 2*i\n",
    "```\n",
    "<br>\n",
    "\n",
    "* Neste caso, você pode iterar sobre todos os exemplos em `X` usando um loop for e adicionar a `loss` de cada iteração a uma variável (`loss_sum`) inicializada fora do loop.\n",
    "\n",
    "* Em seguida, você pode retornar o `total_cost` como `loss_sum` dividido por `m`.\n",
    "\n",
    "* Se você é novo no Python, verifique se o seu código está corretamente indentado com espaços ou tabulações consistentes. Caso contrário, pode produzir uma saída diferente ou gerar um erro `IndentationError: unexpected indent`. \n",
    "     \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as células abaixo para verificar a sua implementação da função `compute_cost` com duas inicializações diferentes dos parâmetros $w$ e $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Calcular e exibir o custo com w e b inicializados a zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print('Custo em w e b iniciais (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo em w e b iniciais (zeros)<b></td>\n",
    "    <td> 0.693 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o custo com w e b não nulos\n",
    "test_w = np.array([0.2, 0.2])\n",
    "test_b = -24.\n",
    "cost = compute_cost(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Custo em w e b de teste (não nulos): {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "# UNIT TESTS\n",
    "compute_cost_test(compute_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo em w e b de teste (não nulos):<b></td>\n",
    "    <td> 0.218 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.5\"></a>\n",
    "### 2.5 Gradiente para Regressão Logística\n",
    "\n",
    "Nesta seção, você implementará o gradiente para regressão logística.\n",
    "\n",
    "Lembre-se de que o algoritmo do gradiente descendente é:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "onde, os parâmetros $b$, $w_j$ são todos atualizados simultaneamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name='ex-03'></a>\n",
    "### Exercício 3\n",
    "\n",
    "Por favor, complete a função `compute_gradient` para calcular $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ a partir das equações (2) e (3) abaixo.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m é o número de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "    \n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ é a previsão do modelo, enquanto $y^{(i)}$ é o rótulo real\n",
    "\n",
    "\n",
    "- **Nota**: Embora este gradiente pareça idêntico ao gradiente da regressão linear, a fórmula é na verdade diferente porque a regressão linear e a logística têm diferentes definições de $f_{\\mathbf{w},b}(x).$\n",
    "\n",
    "Como antes, você pode usar a função sigmoide que implementou acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: compute_gradient\n",
    "def compute_gradient(X, y, w, b, *argv): \n",
    "    \"\"\"\n",
    "    Calcula o gradiente para regressão logística \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,))  valor alvo \n",
    "      w : (ndarray Shape (n,))  valores dos parâmetros do modelo      \n",
    "      b : (scalar)              valor do parâmetro de bias do modelo\n",
    "      *argv : não usado, para compatibilidade com a versão regularizada abaixo\n",
    "    Returns\n",
    "      dj_dw : (ndarray Shape (n,)) O gradiente do custo em relação aos parâmetros w. \n",
    "      dj_db : (scalar)             O gradiente do custo em relação ao parâmetro b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    for i in range(m):\n",
    "        z_wb = None\n",
    "        for j in range(n): \n",
    "            z_wb += None\n",
    "        z_wb += None\n",
    "        f_wb = None\n",
    "        \n",
    "        dj_db_i = None\n",
    "        dj_db += None\n",
    "        \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = None\n",
    "            \n",
    "    dj_dw = None\n",
    "    dj_db = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as células abaixo para verificar a sua implementação da função `compute_gradient` com duas inicializações diferentes dos parâmetros $w$ e $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o gradiente com w e b inicializados a zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
    "print(f'dj_db em w e b iniciais (zeros):{dj_db}' )\n",
    "print(f'dj_dw em w e b iniciais (zeros):{dj_dw.tolist()}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db em w e b iniciais (zeros)<b></td>\n",
    "    <td> -0.1 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>dj_dw em w e b iniciais (zeros):<b></td>\n",
    "    <td> [-12.00921658929115, -11.262842205513591] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o custo e o gradiente com w e b não nulos\n",
    "test_w = np.array([ 0.2, -0.5])\n",
    "test_b = -24\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('dj_db em w e b de teste:', dj_db)\n",
    "print('dj_dw em w e b de teste:', dj_dw.tolist())\n",
    "\n",
    "# UNIT TESTS \n",
    "compute_gradient_test(compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db em w e b de teste (não nulos)<b></td>\n",
    "    <td> -0.5999999999991071 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>dj_dw em w e b de teste (não nulos):<b></td>\n",
    "    <td> [-44.8313536178737957, -44.37384124953978] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.6\"></a>\n",
    "### 2.6 Aprendizagem de parâmetros usando o gradiente descendente \n",
    "\n",
    "Semelhante à tarefa anterior, agora você encontrará os parâmetros ótimos de um modelo de regressão logística usando o gradiente descendente. \n",
    "- Você não precisa implementar nada para esta parte. Simplesmente execute as células abaixo. \n",
    "\n",
    "- Uma boa maneira de verificar se o gradiente descendente está a funcionar corretamente é observar o valor de $J(\\mathbf{w},b)$ e verificar se está a diminuir a cada passo. \n",
    "\n",
    "- Assumindo que você implementou o gradiente e calculou o custo corretamente, o seu valor de $J(\\mathbf{w},b)$ nunca deve aumentar e deve convergir para um valor estável no final do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Executa o gradiente descendente em lote para aprender theta. Atualiza theta dando \n",
    "    num_iters passos do gradiente com taxa de aprendizagem alpha\n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m, n) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,)) valor alvo \n",
    "      w_in : (ndarray Shape (n,)) Valores iniciais dos parâmetros do modelo\n",
    "      b_in : (scalar) Valor inicial do parâmetro do modelo\n",
    "      cost_function : função para calcular o custo\n",
    "      gradient_function : função para calcular o gradiente\n",
    "      alpha : (float) Taxa de aprendizagem\n",
    "      num_iters : (int) número de iterações para executar o gradiente descendente\n",
    "      lambda_ : (scalar, float) constante de regularização\n",
    " \n",
    "    Returns:\n",
    "      w : (ndarray Shape (n,)) Valores atualizados dos parâmetros do modelo após\n",
    "      executar o gradiente descendente\n",
    "      b : (scalar) Valor atualizado do parâmetro do modelo após\n",
    "      executar o gradiente descendente\n",
    "    \"\"\"\n",
    " \n",
    "    # número de exemplos de treinamento\n",
    "    m = len(X)\n",
    " \n",
    "    # Um array para armazenar o custo J e os w's em cada iteração, principalmente para gráficos posteriores\n",
    "    J_history = []\n",
    "    w_history = []\n",
    " \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calcular o gradiente e atualizar os parâmetros\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_) \n",
    "\n",
    "        # Atualizar Parâmetros usando w, b, alpha e gradiente\n",
    "        w_in = w_in - alpha * dj_dw \n",
    "        b_in = b_in - alpha * dj_db \n",
    " \n",
    "        # Salvar custo J em cada iteração\n",
    "        if i<100000: # prevenir o esgotamento de recursos \n",
    "            cost = cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Imprimir o custo a cada intervalo de 10 vezes ou tantas iterações se for < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f} \")\n",
    " \n",
    "    return w_in, b_in, J_history, w_history # retornar w e o histórico de J,w para gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos executar o algoritmo de gradiente descendente acima para aprender os parâmetros para o nosso conjunto de dados.\n",
    "\n",
    "**Nota**\n",
    "O bloco de código abaixo demora alguns minutos a ser executado, especialmente com uma versão não-vetorizada. Você pode reduzir as `iterations` para testar a sua implementação e iterar mais rapidamente. Se tiver tempo mais tarde, tente executar 100.000 iterações para obter melhores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "# Algumas configurações do gradiente descendente\n",
    "iterations = 10000\n",
    "alpha = 0.001\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    " <b>Saída Esperada: Custo 0.30, (Clique para ver detalhes):</b>\n",
    "</summary>\n",
    "\n",
    " # With the following settings\n",
    "    np.random.seed(1)\n",
    "    initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "    initial_b = -8\n",
    "    iterations = 10000\n",
    "    alpha = 0.001\n",
    "    #\n",
    "\n",
    "```\n",
    "Iteration    0: Cost     0.96   \n",
    "Iteration 1000: Cost     0.31   \n",
    "Iteration 2000: Cost     0.30   \n",
    "Iteration 3000: Cost     0.30   \n",
    "Iteration 4000: Cost     0.30   \n",
    "Iteration 5000: Cost     0.30   \n",
    "Iteration 6000: Cost     0.30   \n",
    "Iteration 7000: Cost     0.30   \n",
    "Iteration 8000: Cost     0.30   \n",
    "Iteration 9000: Cost     0.30   \n",
    "Iteration 9999: Cost     0.30   \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.7\"></a>\n",
    "### 2.7 Plotagem da Fronteira de Decisão\n",
    "\n",
    "Agora, a fronteira de decisão aprendida é plotada sobre os dados. \n",
    "Assumindo que você implementou corretamente, você deve ver uma fronteira que separa aproximadamente os exemplos positivos dos negativos. \n",
    "\n",
    "<img src=\"images/figure 2.png\" width=\"450\" height=\"450\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plotar a fronteira de decisão\n",
    "plot_decision_boundary(w, b, X_train, y_train)\n",
    "\n",
    "# Adicionar rótulos\n",
    "plt.ylabel('Nota do Exame 2') \n",
    "plt.xlabel('Nota do Exame 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.8\"></a>\n",
    "### 2.8 Avaliação da Regressão Logística\n",
    "\n",
    "Depois de aprender os parâmetros, você pode usar o modelo para prever se um determinado estudante será admitido.\n",
    "\n",
    "Você irá implementar a função `predict` abaixo para fazer isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name='ex-04'></a>\n",
    "### Exercício 4\n",
    "\n",
    "Complete a função `predict` para produzir predições `1` ou `0` dada um conjunto de dados e parâmetros aprendidos $w$ e $b$.\n",
    "- Primeiro, você precisa computar a predição a partir do modelo $f(x^{(i)}) = g(w \\cdot x^{(i)} + b)$ para cada exemplo \n",
    "    - Você já implementou isso antes nas partes acima\n",
    "- Interpretamos a saída do modelo ($f(x^{(i)})$) como a probabilidade de que $y^{(i)}=1$ dado $x^{(i)}$ e parametrizado por $w$.\n",
    "- Portanto, para obter a predição final ($y^{(i)}=0$ ou $y^{(i)}=1$) a partir do modelo de regressão logística, você pode usar a seguinte heurística -\n",
    "\n",
    "  se $f(x^{(i)}) >= 0.5$, predizer $y^{(i)}=1$\n",
    "  \n",
    "  se $f(x^{(i)}) < 0.5$, predizer $y^{(i)}=0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: predict\n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Prevê se o rótulo é 0 ou 1 usando parâmetros de regressão logística aprendidos (w, b)\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      w : (ndarray Shape (n,)) valores dos parâmetros do modelo      \n",
    "      b : (scalar) valor do parâmetro de bias do modelo\n",
    "    \n",
    "    Returns:\n",
    "      p : (ndarray (m,1)) Vetor de previsões (0 ou 1)\n",
    "    \"\"\"\n",
    "    # número de exemplos de treinamento\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m) \n",
    "   \n",
    "    ### START CODE HERE ### \n",
    "    # Loop sobre cada exemplo\n",
    "    for i in range(m):   \n",
    "        z_wb = None\n",
    "        # Loop sobre cada feature\n",
    "        for j in range(n): \n",
    "            # Adicionar o termo correspondente a z_wb\n",
    "            z_wb += None\n",
    "        \n",
    "        # Adicionar o termo de bias \n",
    "        z_wb += None\n",
    "        \n",
    "        # Calcular a predicao para esse exemplo\n",
    "        f_wb = None\n",
    "\n",
    "        # Aplicar o threshold\n",
    "        p[i] = None\n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    return p   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos testar sua função de previsão e ver a precisão do seu modelo no conjunto de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste seu codigo de predicao\n",
    "np.random.seed(1)\n",
    "tmp_w = np.random.randn(2)\n",
    "tmp_b = 0.3    \n",
    "tmp_X = np.random.randn(4, 2) - 0.5\n",
    "\n",
    "tmp_p = predict(tmp_X, tmp_w, tmp_b)\n",
    "print(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')\n",
    "\n",
    "# UNIT TESTS        \n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada** \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Output of predict: shape (4,),value [0. 1. 1. 1.]<b></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos usar isso para calcular a acurácia do conjunto de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcula a acurácia do nosso conjunto de treinamento\n",
    "p = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Train Accuracy (approx):<b></td>\n",
    "    <td> 92.00 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Regressão Logística Regularizada\n",
    "\n",
    "Nesta parte do exercício, você implementará a regressão logística regularizada para prever se microchips de uma fábrica passam no controle de qualidade (QC).\n",
    "Muitas vezes, a regressão logística com regularização ajuda a evitar o **overfitting** (sobreajuste).\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Declaração do Problema\n",
    "\n",
    "Suponha que você seja o gerente de produto de uma fábrica de microchips e tenha que decidir se um determinado microchip deve ser aceito ou rejeitado. \n",
    "Para cada microchip, você tem os resultados de dois diferentes testes de QC. \n",
    "- Você tem dados históricos de microchips anteriores nos quais você pode basear seu modelo de regressão logística regularizada.\n",
    "- Cada exemplo de treinamento tem os resultados dos dois testes de QC e a decisão (aceito ou rejeitado).\n",
    "\n",
    "Sua tarefa é construir um modelo de classificação que estime a probabilidade de um microchip ser aceito com base nos resultados desses dois testes.\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Carregamento e visualização dos dados\n",
    "\n",
    "Semelhante à primeira parte, vamos começar carregando o conjunto de dados para esta tarefa. \n",
    "- A função `load_data()` mostrada abaixo carrega os dados nas variáveis `X_train` e `y_train`\n",
    "  - `X_train` contém os resultados dos dois testes de QC para um microchip\n",
    "  - `y_train` é a decisão de QC \n",
    "      - `y_train = 1` se o microchip foi aceito\n",
    "      - `y_train = 0` se o microchip foi rejeitado\n",
    "  - Tanto `X_train` quanto `y_train` são arrays numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Carregar conjunto de dados\n",
    "X_train, y_train = load_data(\"data/ex2data2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar as Variáveis\n",
    "\n",
    "O código abaixo exibe os primeiros cinco valores de `X_train` e `y_train` e o tipo das variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print X_train\n",
    "print(\"X_train:\", X_train[:5])\n",
    "print(\"Type of X_train:\",type(X_train))\n",
    "\n",
    "# print y_train\n",
    "print(\"y_train:\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificar as Dimensões de Suas Variáveis\n",
    "\n",
    "Outra maneira útil de se familiarizar com seus dados é visualizar suas dimensões. Vamos imprimir o *shape* (forma/formato) de `X_train` e `y_train` e ver quantos exemplos de treinamento temos em nosso *dataset* (conjunto de dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print ('O formato de X_train é: ' + str(X_train.shape))\n",
    "print ('O formato de y_train é: ' + str(y_train.shape))\n",
    "print ('Temos m = %d exemplos de treinamento' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar seus dados\n",
    "\n",
    "O código abaixo plota os dados num gráfico 2D, onde os eixos são os dois resultados dos testes de QC. \n",
    "\n",
    "Da visualização, parece que você não pode desenhar uma linha reta que separe os exemplos positivos dos negativos. Portanto, uma regressão logística simples não funcionará bem neste conjunto de dados não linear. \n",
    "\n",
    "<img src=\"images/figure 3.png\" width=\"450\" height=\"450\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plotar exemplos\n",
    "plot_data(X_train, y_train[:], pos_label=\"Aceito\", neg_label=\"Rejeitado\")\n",
    "\n",
    "# Definir rótulos\n",
    "plt.ylabel('Resultado do Teste 2') \n",
    "plt.xlabel('Resultado do Teste 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Mapeamento de Features\n",
    "\n",
    "Uma forma de ajustar os dados usando o modelo de regressão logística sem regularização é criar mais features a partir de cada ponto de dados. \n",
    "- A função `map_feature` mostrada abaixo mapeará as features para todos os termos polinomiais de $x_1$ e $x_2$ até a sexta potência, resultando em um vetor de 27 features para cada exemplo. \n",
    "\n",
    "$$\\mathrm{map\\_feature}(x) = \n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_1^2\\\\\n",
    "x_1 x_2\\\\\n",
    "x_2^2\\\\\n",
    "x_1^3\\\\\n",
    "\\vdots\\\\\n",
    "x_1 x_2^5\\\\\n",
    "x_2^6\\end{array}\\right]$$\n",
    "\n",
    "Isso criará uma fronteira de decisão de alta dimensão que é suficiente para separar os exemplos positivos dos negativos.\n",
    "\n",
    "- Você deve usar a função `map_feature` que está no arquivo `utils.py` para mapear os dados e ajustá-los aos parâmetros do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Formato original dos dados\", X_train.shape)\n",
    "\n",
    "mapped_X =  map_feature(X_train[:, 0], X_train[:, 1])\n",
    "print(\"Formato depois do mapeamento de features:\", mapped_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos também imprimir os primeiros elementos de `X_train` e `mapped_X` para ver a transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train[0]:\", X_train[0])\n",
    "print(\"mapped X_train[0]:\", mapped_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora o mapeamento de *features* (características/atributos) nos permita construir um classificador mais expressivo, ele também é mais suscetível ao *overfitting* (sobreajuste). Nas próximas partes do exercício, você implementará a regressão logística regularizada para ajustar os dados e também verá por si mesmo como a regularização pode ajudar a combater o problema do *overfitting*.\n",
    "\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Função Custo para Regressão Logística Regularizada\n",
    "\n",
    "Nesta parte, você implementará a função custo para a regressão logística regularizada.\n",
    "\n",
    "Lembre-se que, para a regressão logística regularizada, a função custo tem a forma:\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "Compare isso com a função custo sem regularização (que você implementou acima), que tem a forma:\n",
    "\n",
    "$$ J(\\mathbf{w}.b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right]$$\n",
    "\n",
    "A diferença é o termo de regularização, que é $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ \n",
    "Note que o parâmetro $b$ não é regularizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-05'></a>\n",
    "### Exercício 5\n",
    "\n",
    "Complete a função `compute_cost_reg` abaixo para calcular o seguinte termo para cada elemento em $w$\n",
    "\n",
    "$$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "O código inicial (ou código base) então adiciona isso ao custo sem regularização (o qual você calculou acima em `compute_cost`) para calcular o custo com regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: compute_cost_reg\n",
    "\n",
    "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computa o custo sobre todos os exemplos com regularização\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,))  valor alvo \n",
    "      w : (ndarray Shape (n,))  valores dos parâmetros do modelo      \n",
    "      b : (scalar)              valor do parâmetro de bias do modelo\n",
    "      lambda_ : (scalar,float)  parâmetro de regularização\n",
    "    \n",
    "    Returns:\n",
    "      total_cost : (scalar)     custo calculado com regularização \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Chamar a função de custo não regularizada\n",
    "    cost_without_reg = compute_cost(X, y, w, b) \n",
    "    \n",
    "    # Você não deve regularizar o termo de bias (b)\n",
    "    reg_cost = 0.\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    total_cost = cost_without_reg + reg_cost\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as células abaixo para verificar a sua implementação da função `compute_cost_reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1]) - 0.5\n",
    "initial_b = 0.5\n",
    "lambda_ = 0.5\n",
    "cost = compute_cost_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "\n",
    "print(\"Custo regularizado :\", cost)\n",
    "\n",
    "# UNIT TEST    \n",
    "compute_cost_reg_test(compute_cost_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo regularizado : <b></td>\n",
    "    <td> 0.6618252552483948 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "### 3.5 Gradiente para Regressão Logística Regularizada\n",
    "\n",
    "Para regressão logística com regularização, o gradiente do custo é definido como:\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\left( \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\right) + \\frac{\\lambda}{m} w_j \\quad\\, \\text{ para } j=0...(n-1) \\tag{5}\n",
    "$$\n",
    "\n",
    "onde $m$ é o número de exemplos de treinamento no conjunto de dados, e $\\lambda$ é o parâmetro de regularização.\n",
    "\n",
    "O termo de bias $b$ não é regularizado. \n",
    "A forma como você calcula o termo de bias é idêntica à forma como o calculou na seção **2.5**. Você também pode notar que a porção não regularizada do gradiente para $w_j$ é idêntica àquela na seção **2.5**, a única diferença é a adição do termo de regularização, $\\frac{\\lambda}{m} w_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-06'></a>\n",
    "### Exercício 6\n",
    "\n",
    "Complete a função `compute_gradient_reg` abaixo para modificar o código e calcular o seguinte termo:\n",
    "\n",
    "$$\\frac{\\lambda}{m} w_j  \\quad\\, \\text{ para } j=0...(n-1)$$\n",
    "\n",
    "O código base adicionará este termo ao $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$ retornado da função `compute_gradient` acima para obter o gradiente da função custo regularizada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6\n",
    "# GRADED FUNCTION: compute_gradient_reg\n",
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Calcula o gradiente para regressão logística com regularização\n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,))  valor alvo \n",
    "      w : (ndarray Shape (n,))  valores dos parâmetros do modelo      \n",
    "      b : (scalar)              valor do parâmetro de bias do modelo\n",
    "      lambda_ : (scalar,float)  parâmetro de regularização\n",
    "    \n",
    "    Returns:\n",
    "      dj_db : (scalar)             O gradiente do custo em relação ao parâmetro b. \n",
    "      dj_dw : (ndarray Shape (n,)) O gradiente do custo em relação aos parâmetros w. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Chamar a função de gradiente não regularizada\n",
    "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "    \n",
    "    ### START CODE HERE ###         \n",
    "    \n",
    "    ### END CODE HERE ###         \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as células abaixo para verificar a sua implementação da função `compute_gradient_reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1) \n",
    "initial_w  = np.random.rand(X_mapped.shape[1]) - 0.5 \n",
    "initial_b = 0.5\n",
    " \n",
    "lambda_ = 0.5\n",
    "dj_db, dj_dw = compute_gradient_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "\n",
    "print(f\"dj_db: {dj_db}\", )\n",
    "print(f\"Primeiros elementos do dj_dw regularizado:\\n {dj_dw[:4].tolist()}\", )\n",
    "\n",
    "# UNIT TESTS    \n",
    "compute_gradient_reg_test(compute_gradient_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db:</b>0.07138288792343</td> </tr>\n",
    "  <tr>\n",
    "      <td> <b> Primeiros elementos do dj_dw regularizado: </b> </td> </tr>\n",
    "   <tr>\n",
    "   <td> [[-0.010386028450548], [0.011409852883280], [0.0536273463274], [0.003140278267313]] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.6\"></a>\n",
    "### 3.6 Aprendizagem de parâmetros usando o gradiente descendente\n",
    "\n",
    "Similarmente às partes anteriores, você usará a sua função de descida do gradiente implementada acima para aprender os parâmetros ideais $w$ e $b$.\n",
    "- Se você completou o cálculo do custo e do gradiente para a regressão logística regularizada corretamente, você deverá ser capaz de executar a próxima célula para aprender os parâmetros $w$.\n",
    "- Após treinar nossos parâmetros, usaremos eles para plotar a fronteira de decisão.\n",
    "\n",
    "\n",
    "**Observação**\n",
    "\n",
    "O bloco de código abaixo leva um tempo considerável para ser executado, especialmente com uma versão não vetorizada. Você pode reduzir as iterações para testar sua implementação e iterar mais rápido. Se tiver tempo depois, execute por 100.000 iterações para ver resultados melhores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1])-0.5\n",
    "initial_b = 1.\n",
    "\n",
    "# Set regularization parameter lambda_ (you can try varying this)\n",
    "lambda_ = 0.01    \n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_mapped, y_train, initial_w, initial_b, \n",
    "                                    compute_cost_reg, compute_gradient_reg, \n",
    "                                    alpha, iterations, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>Saída Esperada: Custo < 0.5  (Clique para detalhes)</b>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "# Using the following settings\n",
    "#np.random.seed(1)\n",
    "#initial_w = np.random.rand(X_mapped.shape[1])-0.5\n",
    "#initial_b = 1.\n",
    "#lambda_ = 0.01;                                          \n",
    "#iterations = 10000\n",
    "#alpha = 0.01\n",
    "Iteration    0: Cost     0.72   \n",
    "Iteration 1000: Cost     0.59   \n",
    "Iteration 2000: Cost     0.56   \n",
    "Iteration 3000: Cost     0.53   \n",
    "Iteration 4000: Cost     0.51   \n",
    "Iteration 5000: Cost     0.50   \n",
    "Iteration 6000: Cost     0.48   \n",
    "Iteration 7000: Cost     0.47   \n",
    "Iteration 8000: Cost     0.46   \n",
    "Iteration 9000: Cost     0.45   \n",
    "Iteration 9999: Cost     0.45       \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.7\"></a>\n",
    "### 3.7 Plotagem da Fronteira de Decisão\n",
    "\n",
    "Para ajudar você a visualizar o modelo aprendido por este classificador, usaremos nossa função `plot_decision_boundary`, que plota a fronteira de decisão (não-linear) que separa os exemplos positivos e negativos.\n",
    "\n",
    "- Dentro da função, plotamos a fronteira de decisão não-linear calculando as previsões do classificador em uma grade uniformemente espaçada e, em seguida, desenhamos um gráfico de contorno onde as previsões mudam de $y = 0$ para $y = 1$.\n",
    "\n",
    "- Após aprender os parâmetros $w$ e $b$, o próximo passo é plotar uma fronteira de decisão semelhante à Figura 4.\n",
    "\n",
    "<img src=\"images/figure 4.png\" width=\"450\" height=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plotar a fronteira de decisão\n",
    "plot_decision_boundary(w, b, X_mapped, y_train)\n",
    "\n",
    "# Adicionar rótulos\n",
    "plt.ylabel('Resultado do Teste 2') \n",
    "plt.xlabel('Resultado do Teste 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8\"></a>\n",
    "### 3.8 Avaliação do modelo de Regressão Logística Regularizada\n",
    "\n",
    "Você usará a função `predict` que você implementou acima para calcular a acurácia do modelo de regressão logística regularizada no conjunto de treinamento (training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular a precisão no conjunto de treinamento\n",
    "p = predict(X_mapped, w, b)\n",
    "\n",
    "print('Precisão do Treinamento: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Precisão do Treinamento<b></td>\n",
    "    <td> ~80% </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
