{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress√£o Log√≠stica\n",
    "\n",
    "Neste trabalho, voc√™ implementar√° a regress√£o log√≠stica e a aplicar√° a dois conjuntos de dados diferentes.\n",
    "\n",
    "# Sum√°rio\n",
    "- [ 1 - Pacotes ](#1)\n",
    "- [ 2 - Regress√£o Log√≠stica](#2)\n",
    "  - [ 2.1 Declara√ß√£o do Problema](#2.1)\n",
    "  - [ 2.2 Carregamento e visualiza√ß√£o dos dados](#2.2)\n",
    "  - [ 2.3 Fun√ß√£o Sigmoide](#2.3)\n",
    "  - [ 2.4 Fun√ß√£o de Custo para Regress√£o Log√≠stica](#2.4)\n",
    "  - [ 2.5 Gradiente para Regress√£o Log√≠stica](#2.5)\n",
    "  - [ 2.6 Aprendizagem de par√¢metros usando o gradiente descendente ](#2.6)\n",
    "  - [ 2.7 Plotagem da Fronteira de Decis√£o](#2.7)\n",
    "  - [ 2.8 Avalia√ß√£o da Regress√£o Log√≠stica](#2.8)\n",
    "- [ 3 - Regress√£o Log√≠stica Regularizada](#3)\n",
    "  - [ 3.1 Declara√ß√£o do Problema](#3.1)\n",
    "  - [ 3.2 Carregamento e visualiza√ß√£o dos dados](#3.2)\n",
    "  - [ 3.3 Mapeamento de Features](#3.3)\n",
    "  - [ 3.4 Fun√ß√£o de Custo para Regress√£o Log√≠stica Regularizada](#3.4)\n",
    "  - [ 3.5 Gradiente para Regress√£o Log√≠stica Regularizada](#3.5)\n",
    "  - [ 3.6 Aprendizagem de par√¢metros usando o gradiente descendente](#3.6)\n",
    "  - [ 3.7 Plotagem da Fronteira de Decis√£o](#3.7)\n",
    "  - [ 3.8 Avalia√ß√£o do modelo de Regress√£o Log√≠stica Regularizada](#3.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** Para evitar erros do corretor autom√°tico, √© apenas permitido editar os c√≥digo com o coment√°rio no in√≠cio `# GRADED FUNCTION` (que s√£o, efetivamente, os 6 c√≥digos que voc√™ deve preencher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Pacotes \n",
    "\n",
    "Primeiro, vamos executar a c√©lula abaixo para importar todos os pacotes de que voc√™ precisar√° durante esta tarefa.\n",
    "- [numpy](www.numpy.org) √© o pacote fundamental para computa√ß√£o cient√≠fica com Python.\n",
    "- [matplotlib](http://matplotlib.org) √© uma biblioteca famosa para plotar gr√°ficos em Python.\n",
    "-  ``utils.py`` cont√©m fun√ß√µes auxiliares para esta tarefa. Voc√™ n√£o precisa modificar o c√≥digo neste arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import copy\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Regress√£o Log√≠stica\n",
    "\n",
    "Nesta parte do exerc√≠cio, voc√™ construir√° um modelo de regress√£o log√≠stica para prever se um estudante ser√° admitido numa universidade.\n",
    "\n",
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Declara√ß√£o do Problema\n",
    "\n",
    "Suponha que voc√™ seja o administrador de um departamento universit√°rio e queira determinar a chance de admiss√£o de cada candidato com base nos seus resultados em dois exames. \n",
    "* Voc√™ tem dados hist√≥ricos de candidatos anteriores que pode usar como um conjunto de treinamento para regress√£o log√≠stica. \n",
    "* Para cada exemplo de treinamento, voc√™ tem as notas do candidato em dois exames e a decis√£o de admiss√£o. \n",
    "* Sua tarefa √© construir um modelo de classifica√ß√£o que estime a probabilidade de admiss√£o de um candidato com base nas notas desses dois exames. \n",
    "\n",
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Carregamento e visualiza√ß√£o dos dados\n",
    "\n",
    "Voc√™ come√ßar√° carregando o conjunto de dados para esta tarefa. \n",
    "- A fun√ß√£o `load_data()` mostrada abaixo carrega os dados nas vari√°veis `X_train` e `y_train`\n",
    "  - `X_train` cont√©m as notas em dois exames para um estudante\n",
    "  - `y_train` √© a decis√£o de admiss√£o \n",
    "      - `y_train = 1` se o estudante foi admitido \n",
    "      - `y_train = 0` se o estudante n√£o foi admitido \n",
    "  - Tanto `X_train` quanto `y_train` s√£o arrays numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# carregar conjunto de dados\n",
    "X_train, y_train = load_data(\"data/ex2data1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar as vari√°veis\n",
    "Vamos nos familiarizar mais com o seu conjunto de dados.  \n",
    "- Um bom ponto de partida √© simplesmente imprimir cada vari√°vel e ver o que ela cont√©m.\n",
    "\n",
    "O c√≥digo abaixo imprime os primeiros cinco valores de `X_train` e o tipo da vari√°vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os primeiros cinco elementos em X_train s√£o:\n",
      " [[34.62365962 78.02469282]\n",
      " [30.28671077 43.89499752]\n",
      " [35.84740877 72.90219803]\n",
      " [60.18259939 86.3085521 ]\n",
      " [79.03273605 75.34437644]]\n",
      "Tipo de X_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Os primeiros cinco elementos em X_train s√£o:\\n\", X_train[:5])\n",
    "print(\"Tipo de X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora imprima os primeiros cinco valores de `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os primeiros cinco elementos em y_train s√£o:\n",
      " [0. 0. 0. 1. 1.]\n",
      "Tipo de y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Os primeiros cinco elementos em y_train s√£o:\\n\", y_train[:5])\n",
    "print(\"Tipo de y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificar as dimens√µes das suas vari√°veis\n",
    "\n",
    "Outra forma √∫til de se familiarizar com seus dados √© visualizar suas dimens√µes. Vamos imprimir o formato de `X_train` e `y_train` e ver quantos exemplos de treinamento temos em nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O formato de X_train √©: (100, 2)\n",
      "O formato de y_train √©: (100,)\n",
      "Temos m = 100 exemplos de treinamento\n"
     ]
    }
   ],
   "source": [
    "print ('O formato de X_train √©: ' + str(X_train.shape))\n",
    "print ('O formato de y_train √©: ' + str(y_train.shape))\n",
    "print ('Temos m = %d exemplos de treinamento' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar seus dados\n",
    "\n",
    "Antes de come√ßar a implementar qualquer algoritmo de aprendizagem, √© sempre bom visualizar os dados, se poss√≠vel.\n",
    "- O c√≥digo abaixo exibe os dados num gr√°fico 2D (como mostrado abaixo), onde os eixos s√£o as notas dos dois exames, e os exemplos positivos e negativos s√£o mostrados com marcadores diferentes.\n",
    "- Usamos uma fun√ß√£o auxiliar no arquivo ``utils.py`` para gerar este gr√°fico. \n",
    "\n",
    "<img src=\"images/figure 1.png\" width=\"450\" height=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotar exemplos\n",
    "plot_data(X_train, y_train[:], pos_label=\"Admitido\", neg_label=\"N√£o admitido\")\n",
    "\n",
    "# Definir o r√≥tulo do eixo y\n",
    "plt.ylabel('Nota do Exame 2') \n",
    "# Definir o r√≥tulo do eixo x\n",
    "plt.xlabel('Nota do Exame 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seu objetivo √© construir um modelo de regress√£o log√≠stica para ajustar estes dados.\n",
    "- Com este modelo, voc√™ pode ent√£o prever se um novo estudante ser√° admitido com base nas suas notas nos dois exames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.3\"></a>\n",
    "### 2.3 Fun√ß√£o Sigmoide\n",
    "\n",
    "Lembre-se de que para a regress√£o log√≠stica, o modelo √© representado como\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "onde a fun√ß√£o $g$ √© a fun√ß√£o sigmoide. A fun√ß√£o sigmoide √© definida como:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Vamos implementar a fun√ß√£o sigmoide primeiro, para que possa ser usada no resto desta tarefa.\n",
    "\n",
    "<a name='ex-01'></a>\n",
    "### Exerc√≠cio 1\n",
    "Por favor, complete a fun√ß√£o `sigmoid` para calcular\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Note que \n",
    "- `z` nem sempre √© um √∫nico n√∫mero, mas tamb√©m pode ser um array de n√∫meros. \n",
    "- Se a entrada for um array de n√∫meros, gostar√≠amos de aplicar a fun√ß√£o sigmoide a cada valor no array de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcula o sigmoide de z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): Um escalar, array numpy de qualquer tamanho.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), com o mesmo formato que z\n",
    "         \n",
    "    \"\"\"\n",
    "          \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    g = 1 / (1 + np.exp(-z)) \n",
    "    \n",
    "    ### END SOLUTION ###  \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando terminar, tente testar alguns valores chamando `sigmoid(x)` na c√©lula abaixo. \n",
    "- Para valores grandes e positivos de x, o sigmoide deve estar perto de 1, enquanto para valores grandes e negativos, o sigmoide deve estar perto de 0. \n",
    "- Avaliar `sigmoid(0)` deve dar exatamente 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Nota: Voc√™ pode editar este valor\n",
    "value = 0\n",
    "\n",
    "print (f\"sigmoid({value}) = {sigmoid(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>sigmoid(0)<b></td>\n",
    "    <td> 0.5 </td> \n",
    "  </tr>\n",
    "</table>\n",
    "    \n",
    "- Como mencionado antes, seu c√≥digo tamb√©m deve funcionar com vetores e matrizes. Para uma matriz, sua fun√ß√£o deve executar a fun√ß√£o sigmoide em cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([ -1, 0, 1, 2]) = [0.26894142 0.5        0.73105858 0.88079708]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([ -1, 0, 1, 2]) = \" + str(sigmoid(np.array([-1, 0, 1, 2]))))\n",
    "\n",
    "# UNIT TESTS\n",
    "from public_tests import *\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b>sigmoid([-1, 0, 1, 2])<b></td> \n",
    "    <td>[0.26894142        0.5           0.73105858        0.88079708]</td> \n",
    "  </tr>    \n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.4\"></a>\n",
    "### 2.4 Fun√ß√£o de Custo para Regress√£o Log√≠stica\n",
    "\n",
    "Nesta se√ß√£o, voc√™ implementar√° a fun√ß√£o de custo para regress√£o log√≠stica.\n",
    "\n",
    "<a name='ex-02'></a>\n",
    "### Exerc√≠cio 2\n",
    "\n",
    "Por favor, complete a fun√ß√£o `compute_cost` usando as equa√ß√µes abaixo.\n",
    "\n",
    "Lembre-se que para a regress√£o log√≠stica, a fun√ß√£o de custo tem a forma \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "onde\n",
    "* m √© o n√∫mero de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ √© o custo para um √∫nico ponto de dados, que √© - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ √© a previs√£o do modelo, enquanto $y^{(i)}$, que √© o r√≥tulo real\n",
    "\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ onde a fun√ß√£o $g$ √© a fun√ß√£o sigmoide.\n",
    "    * Pode ser √∫til calcular primeiro uma vari√°vel intermedi√°ria $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ onde $n$ √© o n√∫mero de features, antes de calcular $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$\n",
    "\n",
    "Nota:\n",
    "* Ao fazer isso, lembre-se de que as vari√°veis `X_train` e `y_train` n√£o s√£o valores escalares, mas sim matrizes de formato ($m, n$) e ($ùëö$,1) respetivamente, onde $ùëõ$ √© o n√∫mero de features e $ùëö$ √© o n√∫mero de exemplos de treinamento.\n",
    "* Voc√™ pode usar a fun√ß√£o sigmoide que implementou acima para esta parte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_cost\n",
    "def compute_cost(X, y, w, b, *argv):\n",
    "    \"\"\"\n",
    "    Calcula o custo sobre todos os exemplos\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,))  valor alvo \n",
    "      w : (ndarray Shape (n,))  valores dos par√¢metros do modelo      \n",
    "      b : (scalar)              valor do par√¢metro de bias do modelo\n",
    "      *argv : n√£o usado, para compatibilidade com a vers√£o regularizada abaixo\n",
    "    Returns:\n",
    "      total_cost : (scalar) custo \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><font size=\"3\" color=\"darkgreen\"><b>Clique para ver as dicas</b></font></summary>\n",
    "    \n",
    "* Voc√™ pode representar um operador de somat√≥rio eg: $h = \\sum\\limits_{i = 0}^{m-1} 2i$ em c√≥digo da seguinte forma:\n",
    "\n",
    "```python\n",
    "    h = 0\n",
    "    for i in range(m):\n",
    "        h = h + 2*i\n",
    "```\n",
    "<br>\n",
    "\n",
    "* Neste caso, voc√™ pode iterar sobre todos os exemplos em `X` usando um loop for e adicionar a `loss` de cada itera√ß√£o a uma vari√°vel (`loss_sum`) inicializada fora do loop.\n",
    "\n",
    "* Em seguida, voc√™ pode retornar o `total_cost` como `loss_sum` dividido por `m`.\n",
    "\n",
    "* Se voc√™ √© novo no Python, verifique se o seu c√≥digo est√° corretamente indentado com espa√ßos ou tabula√ß√µes consistentes. Caso contr√°rio, pode produzir uma sa√≠da diferente ou gerar um erro `IndentationError: unexpected indent`. \n",
    "     \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as c√©lulas abaixo para verificar a sua implementa√ß√£o da fun√ß√£o `compute_cost` com duas inicializa√ß√µes diferentes dos par√¢metros $w$ e $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Calcular e exibir o custo com w e b inicializados a zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print('Custo em w e b iniciais (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo em w e b iniciais (zeros)<b></td>\n",
    "    <td> 0.693 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o custo com w e b n√£o nulos\n",
    "test_w = np.array([0.2, 0.2])\n",
    "test_b = -24.\n",
    "cost = compute_cost(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Custo em w e b de teste (n√£o nulos): {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "# UNIT TESTS\n",
    "compute_cost_test(compute_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo em w e b de teste (n√£o nulos):<b></td>\n",
    "    <td> 0.218 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.5\"></a>\n",
    "### 2.5 Gradiente para Regress√£o Log√≠stica\n",
    "\n",
    "Nesta se√ß√£o, voc√™ implementar√° o gradiente para regress√£o log√≠stica.\n",
    "\n",
    "Lembre-se de que o algoritmo do gradiente descendente √©:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "onde, os par√¢metros $b$, $w_j$ s√£o todos atualizados simultaneamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name='ex-03'></a>\n",
    "### Exerc√≠cio 3\n",
    "\n",
    "Por favor, complete a fun√ß√£o `compute_gradient` para calcular $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ a partir das equa√ß√µes (2) e (3) abaixo.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m √© o n√∫mero de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "    \n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ √© a previs√£o do modelo, enquanto $y^{(i)}$ √© o r√≥tulo real\n",
    "\n",
    "\n",
    "- **Nota**: Embora este gradiente pare√ßa id√™ntico ao gradiente da regress√£o linear, a f√≥rmula √© na verdade diferente porque a regress√£o linear e a log√≠stica t√™m diferentes defini√ß√µes de $f_{\\mathbf{w},b}(x).$\n",
    "\n",
    "Como antes, voc√™ pode usar a fun√ß√£o sigmoide que implementou acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: compute_gradient\n",
    "def compute_gradient(X, y, w, b, *argv): \n",
    "    \"\"\"\n",
    "    Calcula o gradiente para regress√£o log√≠stica \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,))  valor alvo \n",
    "      w : (ndarray Shape (n,))  valores dos par√¢metros do modelo      \n",
    "      b : (scalar)              valor do par√¢metro de bias do modelo\n",
    "      *argv : n√£o usado, para compatibilidade com a vers√£o regularizada abaixo\n",
    "    Returns\n",
    "      dj_dw : (ndarray Shape (n,)) O gradiente do custo em rela√ß√£o aos par√¢metros w. \n",
    "      dj_db : (scalar)             O gradiente do custo em rela√ß√£o ao par√¢metro b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    for i in range(m):\n",
    "        z_wb = None\n",
    "        for j in range(n): \n",
    "            z_wb += None\n",
    "        z_wb += None\n",
    "        f_wb = None\n",
    "        \n",
    "        dj_db_i = None\n",
    "        dj_db += None\n",
    "        \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = None\n",
    "            \n",
    "    dj_dw = None\n",
    "    dj_db = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as c√©lulas abaixo para verificar a sua implementa√ß√£o da fun√ß√£o `compute_gradient` com duas inicializa√ß√µes diferentes dos par√¢metros $w$ e $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o gradiente com w e b inicializados a zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
    "print(f'dj_db em w e b iniciais (zeros):{dj_db}' )\n",
    "print(f'dj_dw em w e b iniciais (zeros):{dj_dw.tolist()}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db em w e b iniciais (zeros)<b></td>\n",
    "    <td> -0.1 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>dj_dw em w e b iniciais (zeros):<b></td>\n",
    "    <td> [-12.00921658929115, -11.262842205513591] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o custo e o gradiente com w e b n√£o nulos\n",
    "test_w = np.array([ 0.2, -0.5])\n",
    "test_b = -24\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('dj_db em w e b de teste:', dj_db)\n",
    "print('dj_dw em w e b de teste:', dj_dw.tolist())\n",
    "\n",
    "# UNIT TESTS \n",
    "compute_gradient_test(compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db em w e b de teste (n√£o nulos)<b></td>\n",
    "    <td> -0.5999999999991071 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>dj_dw em w e b de teste (n√£o nulos):<b></td>\n",
    "    <td> [-44.8313536178737957, -44.37384124953978] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.6\"></a>\n",
    "### 2.6 Aprendizagem de par√¢metros usando o gradiente descendente \n",
    "\n",
    "Semelhante √† tarefa anterior, agora voc√™ encontrar√° os par√¢metros √≥timos de um modelo de regress√£o log√≠stica usando o gradiente descendente. \n",
    "- Voc√™ n√£o precisa implementar nada para esta parte. Simplesmente execute as c√©lulas abaixo. \n",
    "\n",
    "- Uma boa maneira de verificar se o gradiente descendente est√° a funcionar corretamente √© observar o valor de $J(\\mathbf{w},b)$ e verificar se est√° a diminuir a cada passo. \n",
    "\n",
    "- Assumindo que voc√™ implementou o gradiente e calculou o custo corretamente, o seu valor de $J(\\mathbf{w},b)$ nunca deve aumentar e deve convergir para um valor est√°vel no final do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Executa o gradiente descendente em lote para aprender theta. Atualiza theta dando \n",
    "    num_iters passos do gradiente com taxa de aprendizagem alpha\n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m, n) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,)) valor alvo \n",
    "      w_in : (ndarray Shape (n,)) Valores iniciais dos par√¢metros do modelo\n",
    "      b_in : (scalar) Valor inicial do par√¢metro do modelo\n",
    "      cost_function : fun√ß√£o para calcular o custo\n",
    "      gradient_function : fun√ß√£o para calcular o gradiente\n",
    "      alpha : (float) Taxa de aprendizagem\n",
    "      num_iters : (int) n√∫mero de itera√ß√µes para executar o gradiente descendente\n",
    "      lambda_ : (scalar, float) constante de regulariza√ß√£o\n",
    " \n",
    "    Returns:\n",
    "      w : (ndarray Shape (n,)) Valores atualizados dos par√¢metros do modelo ap√≥s\n",
    "      executar o gradiente descendente\n",
    "      b : (scalar) Valor atualizado do par√¢metro do modelo ap√≥s\n",
    "      executar o gradiente descendente\n",
    "    \"\"\"\n",
    " \n",
    "    # n√∫mero de exemplos de treinamento\n",
    "    m = len(X)\n",
    " \n",
    "    # Um array para armazenar o custo J e os w's em cada itera√ß√£o, principalmente para gr√°ficos posteriores\n",
    "    J_history = []\n",
    "    w_history = []\n",
    " \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calcular o gradiente e atualizar os par√¢metros\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_) \n",
    "\n",
    "        # Atualizar Par√¢metros usando w, b, alpha e gradiente\n",
    "        w_in = w_in - alpha * dj_dw \n",
    "        b_in = b_in - alpha * dj_db \n",
    " \n",
    "        # Salvar custo J em cada itera√ß√£o\n",
    "        if i<100000: # prevenir o esgotamento de recursos \n",
    "            cost = cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Imprimir o custo a cada intervalo de 10 vezes ou tantas itera√ß√µes se for < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f} \")\n",
    " \n",
    "    return w_in, b_in, J_history, w_history # retornar w e o hist√≥rico de J,w para gr√°ficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos executar o algoritmo de gradiente descendente acima para aprender os par√¢metros para o nosso conjunto de dados.\n",
    "\n",
    "**Nota**\n",
    "O bloco de c√≥digo abaixo demora alguns minutos a ser executado, especialmente com uma vers√£o n√£o-vetorizada. Voc√™ pode reduzir as `iterations` para testar a sua implementa√ß√£o e iterar mais rapidamente. Se tiver tempo mais tarde, tente executar 100.000 itera√ß√µes para obter melhores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "# Algumas configura√ß√µes do gradiente descendente\n",
    "iterations = 10000\n",
    "alpha = 0.001\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    " <b>Sa√≠da Esperada: Custo 0.30, (Clique para ver detalhes):</b>\n",
    "</summary>\n",
    "\n",
    " # With the following settings\n",
    "    np.random.seed(1)\n",
    "    initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "    initial_b = -8\n",
    "    iterations = 10000\n",
    "    alpha = 0.001\n",
    "    #\n",
    "\n",
    "```\n",
    "Iteration    0: Cost     0.96   \n",
    "Iteration 1000: Cost     0.31   \n",
    "Iteration 2000: Cost     0.30   \n",
    "Iteration 3000: Cost     0.30   \n",
    "Iteration 4000: Cost     0.30   \n",
    "Iteration 5000: Cost     0.30   \n",
    "Iteration 6000: Cost     0.30   \n",
    "Iteration 7000: Cost     0.30   \n",
    "Iteration 8000: Cost     0.30   \n",
    "Iteration 9000: Cost     0.30   \n",
    "Iteration 9999: Cost     0.30   \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.7\"></a>\n",
    "### 2.7 Plotagem da Fronteira de Decis√£o\n",
    "\n",
    "Agora, a fronteira de decis√£o aprendida √© plotada sobre os dados. \n",
    "Assumindo que voc√™ implementou corretamente, voc√™ deve ver uma fronteira que separa aproximadamente os exemplos positivos dos negativos. \n",
    "\n",
    "<img src=\"images/figure 2.png\" width=\"450\" height=\"450\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plotar a fronteira de decis√£o\n",
    "plot_decision_boundary(w, b, X_train, y_train)\n",
    "\n",
    "# Adicionar r√≥tulos\n",
    "plt.ylabel('Nota do Exame 2') \n",
    "plt.xlabel('Nota do Exame 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.8\"></a>\n",
    "### 2.8 Avalia√ß√£o da Regress√£o Log√≠stica\n",
    "\n",
    "Depois de aprender os par√¢metros, voc√™ pode usar o modelo para prever se um determinado estudante ser√° admitido.\n",
    "\n",
    "Voc√™ ir√° implementar a fun√ß√£o `predict` abaixo para fazer isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name='ex-04'></a>\n",
    "### Exerc√≠cio 4\n",
    "\n",
    "Complete a fun√ß√£o `predict` para produzir predi√ß√µes `1` ou `0` dada um conjunto de dados e par√¢metros aprendidos $w$ e $b$.\n",
    "- Primeiro, voc√™ precisa computar a predi√ß√£o a partir do modelo $f(x^{(i)}) = g(w \\cdot x^{(i)} + b)$ para cada exemplo \n",
    "    - Voc√™ j√° implementou isso antes nas partes acima\n",
    "- Interpretamos a sa√≠da do modelo ($f(x^{(i)})$) como a probabilidade de que $y^{(i)}=1$ dado $x^{(i)}$ e parametrizado por $w$.\n",
    "- Portanto, para obter a predi√ß√£o final ($y^{(i)}=0$ ou $y^{(i)}=1$) a partir do modelo de regress√£o log√≠stica, voc√™ pode usar a seguinte heur√≠stica -\n",
    "\n",
    "  se $f(x^{(i)}) >= 0.5$, predizer $y^{(i)}=1$\n",
    "  \n",
    "  se $f(x^{(i)}) < 0.5$, predizer $y^{(i)}=0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: predict\n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Prev√™ se o r√≥tulo √© 0 ou 1 usando par√¢metros de regress√£o log√≠stica aprendidos (w, b)\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      w : (ndarray Shape (n,)) valores dos par√¢metros do modelo      \n",
    "      b : (scalar) valor do par√¢metro de bias do modelo\n",
    "    \n",
    "    Returns:\n",
    "      p : (ndarray (m,1)) Vetor de previs√µes (0 ou 1)\n",
    "    \"\"\"\n",
    "    # n√∫mero de exemplos de treinamento\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m) \n",
    "   \n",
    "    ### START CODE HERE ### \n",
    "    # Loop sobre cada exemplo\n",
    "    for i in range(m):   \n",
    "        z_wb = None\n",
    "        # Loop sobre cada feature\n",
    "        for j in range(n): \n",
    "            # Adicionar o termo correspondente a z_wb\n",
    "            z_wb += None\n",
    "        \n",
    "        # Adicionar o termo de bias \n",
    "        z_wb += None\n",
    "        \n",
    "        # Calcular a predicao para esse exemplo\n",
    "        f_wb = None\n",
    "\n",
    "        # Aplicar o threshold\n",
    "        p[i] = None\n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    return p   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos testar sua fun√ß√£o de previs√£o e ver a precis√£o do seu modelo no conjunto de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste seu codigo de predicao\n",
    "np.random.seed(1)\n",
    "tmp_w = np.random.randn(2)\n",
    "tmp_b = 0.3    \n",
    "tmp_X = np.random.randn(4, 2) - 0.5\n",
    "\n",
    "tmp_p = predict(tmp_X, tmp_w, tmp_b)\n",
    "print(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')\n",
    "\n",
    "# UNIT TESTS        \n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da esperada** \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Output of predict: shape (4,),value [0. 1. 1. 1.]<b></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos usar isso para calcular a acur√°cia do conjunto de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcula a acur√°cia do nosso conjunto de treinamento\n",
    "p = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Train Accuracy (approx):<b></td>\n",
    "    <td> 92.00 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Regress√£o Log√≠stica Regularizada\n",
    "\n",
    "Nesta parte do exerc√≠cio, voc√™ implementar√° a regress√£o log√≠stica regularizada para prever se microchips de uma f√°brica passam no controle de qualidade (QC).\n",
    "Muitas vezes, a regress√£o log√≠stica com regulariza√ß√£o ajuda a evitar o **overfitting** (sobreajuste).\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Declara√ß√£o do Problema\n",
    "\n",
    "Suponha que voc√™ seja o gerente de produto de uma f√°brica de microchips e tenha que decidir se um determinado microchip deve ser aceito ou rejeitado. \n",
    "Para cada microchip, voc√™ tem os resultados de dois diferentes testes de QC. \n",
    "- Voc√™ tem dados hist√≥ricos de microchips anteriores nos quais voc√™ pode basear seu modelo de regress√£o log√≠stica regularizada.\n",
    "- Cada exemplo de treinamento tem os resultados dos dois testes de QC e a decis√£o (aceito ou rejeitado).\n",
    "\n",
    "Sua tarefa √© construir um modelo de classifica√ß√£o que estime a probabilidade de um microchip ser aceito com base nos resultados desses dois testes.\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Carregamento e visualiza√ß√£o dos dados\n",
    "\n",
    "Semelhante √† primeira parte, vamos come√ßar carregando o conjunto de dados para esta tarefa. \n",
    "- A fun√ß√£o `load_data()` mostrada abaixo carrega os dados nas vari√°veis `X_train` e `y_train`\n",
    "  - `X_train` cont√©m os resultados dos dois testes de QC para um microchip\n",
    "  - `y_train` √© a decis√£o de QC \n",
    "      - `y_train = 1` se o microchip foi aceito\n",
    "      - `y_train = 0` se o microchip foi rejeitado\n",
    "  - Tanto `X_train` quanto `y_train` s√£o arrays numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Carregar conjunto de dados\n",
    "X_train, y_train = load_data(\"data/ex2data2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar as Vari√°veis\n",
    "\n",
    "O c√≥digo abaixo exibe os primeiros cinco valores de `X_train` e `y_train` e o tipo das vari√°veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print X_train\n",
    "print(\"X_train:\", X_train[:5])\n",
    "print(\"Type of X_train:\",type(X_train))\n",
    "\n",
    "# print y_train\n",
    "print(\"y_train:\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificar as Dimens√µes de Suas Vari√°veis\n",
    "\n",
    "Outra maneira √∫til de se familiarizar com seus dados √© visualizar suas dimens√µes. Vamos imprimir o *shape* (forma/formato) de `X_train` e `y_train` e ver quantos exemplos de treinamento temos em nosso *dataset* (conjunto de dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print ('O formato de X_train √©: ' + str(X_train.shape))\n",
    "print ('O formato de y_train √©: ' + str(y_train.shape))\n",
    "print ('Temos m = %d exemplos de treinamento' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar seus dados\n",
    "\n",
    "O c√≥digo abaixo plota os dados num gr√°fico 2D, onde os eixos s√£o os dois resultados dos testes de QC. \n",
    "\n",
    "Da visualiza√ß√£o, parece que voc√™ n√£o pode desenhar uma linha reta que separe os exemplos positivos dos negativos. Portanto, uma regress√£o log√≠stica simples n√£o funcionar√° bem neste conjunto de dados n√£o linear. \n",
    "\n",
    "<img src=\"images/figure 3.png\" width=\"450\" height=\"450\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plotar exemplos\n",
    "plot_data(X_train, y_train[:], pos_label=\"Aceito\", neg_label=\"Rejeitado\")\n",
    "\n",
    "# Definir r√≥tulos\n",
    "plt.ylabel('Resultado do Teste 2') \n",
    "plt.xlabel('Resultado do Teste 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Mapeamento de Features\n",
    "\n",
    "Uma forma de ajustar os dados usando o modelo de regress√£o log√≠stica sem regulariza√ß√£o √© criar mais features a partir de cada ponto de dados. \n",
    "- A fun√ß√£o `map_feature` mostrada abaixo mapear√° as features para todos os termos polinomiais de $x_1$ e $x_2$ at√© a sexta pot√™ncia, resultando em um vetor de 27 features para cada exemplo. \n",
    "\n",
    "$$\\mathrm{map\\_feature}(x) = \n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_1^2\\\\\n",
    "x_1 x_2\\\\\n",
    "x_2^2\\\\\n",
    "x_1^3\\\\\n",
    "\\vdots\\\\\n",
    "x_1 x_2^5\\\\\n",
    "x_2^6\\end{array}\\right]$$\n",
    "\n",
    "Isso criar√° uma fronteira de decis√£o de alta dimens√£o que √© suficiente para separar os exemplos positivos dos negativos.\n",
    "\n",
    "- Voc√™ deve usar a fun√ß√£o `map_feature` que est√° no arquivo `utils.py` para mapear os dados e ajust√°-los aos par√¢metros do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Formato original dos dados\", X_train.shape)\n",
    "\n",
    "mapped_X =  map_feature(X_train[:, 0], X_train[:, 1])\n",
    "print(\"Formato depois do mapeamento de features:\", mapped_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tamb√©m imprimir os primeiros elementos de `X_train` e `mapped_X` para ver a transforma√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train[0]:\", X_train[0])\n",
    "print(\"mapped X_train[0]:\", mapped_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora o mapeamento de *features* (caracter√≠sticas/atributos) nos permita construir um classificador mais expressivo, ele tamb√©m √© mais suscet√≠vel ao *overfitting* (sobreajuste). Nas pr√≥ximas partes do exerc√≠cio, voc√™ implementar√° a regress√£o log√≠stica regularizada para ajustar os dados e tamb√©m ver√° por si mesmo como a regulariza√ß√£o pode ajudar a combater o problema do *overfitting*.\n",
    "\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Fun√ß√£o Custo para Regress√£o Log√≠stica Regularizada\n",
    "\n",
    "Nesta parte, voc√™ implementar√° a fun√ß√£o custo para a regress√£o log√≠stica regularizada.\n",
    "\n",
    "Lembre-se que, para a regress√£o log√≠stica regularizada, a fun√ß√£o custo tem a forma:\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "Compare isso com a fun√ß√£o custo sem regulariza√ß√£o (que voc√™ implementou acima), que tem a forma:\n",
    "\n",
    "$$ J(\\mathbf{w}.b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right]$$\n",
    "\n",
    "A diferen√ßa √© o termo de regulariza√ß√£o, que √© $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ \n",
    "Note que o par√¢metro $b$ n√£o √© regularizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-05'></a>\n",
    "### Exerc√≠cio 5\n",
    "\n",
    "Complete a fun√ß√£o `compute_cost_reg` abaixo para calcular o seguinte termo para cada elemento em $w$\n",
    "\n",
    "$$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "O c√≥digo inicial (ou c√≥digo base) ent√£o adiciona isso ao custo sem regulariza√ß√£o (o qual voc√™ calculou acima em `compute_cost`) para calcular o custo com regulariza√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: compute_cost_reg\n",
    "\n",
    "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computa o custo sobre todos os exemplos com regulariza√ß√£o\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,))  valor alvo \n",
    "      w : (ndarray Shape (n,))  valores dos par√¢metros do modelo      \n",
    "      b : (scalar)              valor do par√¢metro de bias do modelo\n",
    "      lambda_ : (scalar,float)  par√¢metro de regulariza√ß√£o\n",
    "    \n",
    "    Returns:\n",
    "      total_cost : (scalar)     custo calculado com regulariza√ß√£o \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Chamar a fun√ß√£o de custo n√£o regularizada\n",
    "    cost_without_reg = compute_cost(X, y, w, b) \n",
    "    \n",
    "    # Voc√™ n√£o deve regularizar o termo de bias (b)\n",
    "    reg_cost = 0.\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    total_cost = cost_without_reg + reg_cost\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as c√©lulas abaixo para verificar a sua implementa√ß√£o da fun√ß√£o `compute_cost_reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1]) - 0.5\n",
    "initial_b = 0.5\n",
    "lambda_ = 0.5\n",
    "cost = compute_cost_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "\n",
    "print(\"Custo regularizado :\", cost)\n",
    "\n",
    "# UNIT TEST    \n",
    "compute_cost_reg_test(compute_cost_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo regularizado : <b></td>\n",
    "    <td> 0.6618252552483948 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "### 3.5 Gradiente para Regress√£o Log√≠stica Regularizada\n",
    "\n",
    "Para regress√£o log√≠stica com regulariza√ß√£o, o gradiente do custo √© definido como:\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\left( \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\right) + \\frac{\\lambda}{m} w_j \\quad\\, \\text{ para } j=0...(n-1) \\tag{5}\n",
    "$$\n",
    "\n",
    "onde $m$ √© o n√∫mero de exemplos de treinamento no conjunto de dados, e $\\lambda$ √© o par√¢metro de regulariza√ß√£o.\n",
    "\n",
    "O termo de bias $b$ n√£o √© regularizado. \n",
    "A forma como voc√™ calcula o termo de bias √© id√™ntica √† forma como o calculou na se√ß√£o **2.5**. Voc√™ tamb√©m pode notar que a por√ß√£o n√£o regularizada do gradiente para $w_j$ √© id√™ntica √†quela na se√ß√£o **2.5**, a √∫nica diferen√ßa √© a adi√ß√£o do termo de regulariza√ß√£o, $\\frac{\\lambda}{m} w_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-06'></a>\n",
    "### Exerc√≠cio 6\n",
    "\n",
    "Complete a fun√ß√£o `compute_gradient_reg` abaixo para modificar o c√≥digo e calcular o seguinte termo:\n",
    "\n",
    "$$\\frac{\\lambda}{m} w_j ¬†\\quad\\, \\text{ para } j=0...(n-1)$$\n",
    "\n",
    "O c√≥digo base adicionar√° este termo ao $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$ retornado da fun√ß√£o `compute_gradient` acima para obter o gradiente da fun√ß√£o custo regularizada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6\n",
    "# GRADED FUNCTION: compute_gradient_reg\n",
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Calcula o gradiente para regress√£o log√≠stica com regulariza√ß√£o\n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos por n features\n",
    "      y : (ndarray Shape (m,))  valor alvo \n",
    "      w : (ndarray Shape (n,))  valores dos par√¢metros do modelo      \n",
    "      b : (scalar)              valor do par√¢metro de bias do modelo\n",
    "      lambda_ : (scalar,float)  par√¢metro de regulariza√ß√£o\n",
    "    \n",
    "    Returns:\n",
    "      dj_db : (scalar)             O gradiente do custo em rela√ß√£o ao par√¢metro b. \n",
    "      dj_dw : (ndarray Shape (n,)) O gradiente do custo em rela√ß√£o aos par√¢metros w. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Chamar a fun√ß√£o de gradiente n√£o regularizada\n",
    "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "    \n",
    "    ### START CODE HERE ###         \n",
    "    \n",
    "    ### END CODE HERE ###         \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as c√©lulas abaixo para verificar a sua implementa√ß√£o da fun√ß√£o `compute_gradient_reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1) \n",
    "initial_w  = np.random.rand(X_mapped.shape[1]) - 0.5 \n",
    "initial_b = 0.5\n",
    " \n",
    "lambda_ = 0.5\n",
    "dj_db, dj_dw = compute_gradient_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "\n",
    "print(f\"dj_db: {dj_db}\", )\n",
    "print(f\"Primeiros elementos do dj_dw regularizado:\\n {dj_dw[:4].tolist()}\", )\n",
    "\n",
    "# UNIT TESTS    \n",
    "compute_gradient_reg_test(compute_gradient_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db:</b>0.07138288792343</td> </tr>\n",
    "  <tr>\n",
    "      <td> <b> Primeiros elementos do dj_dw regularizado: </b> </td> </tr>\n",
    "   <tr>\n",
    "   <td> [[-0.010386028450548], [0.011409852883280], [0.0536273463274], [0.003140278267313]] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.6\"></a>\n",
    "### 3.6 Aprendizagem de par√¢metros usando o gradiente descendente\n",
    "\n",
    "Similarmente √†s partes anteriores, voc√™ usar√° a sua fun√ß√£o de descida do gradiente implementada acima para aprender os par√¢metros ideais $w$ e $b$.\n",
    "- Se voc√™ completou o c√°lculo do custo e do gradiente para a regress√£o log√≠stica regularizada corretamente, voc√™ dever√° ser capaz de executar a pr√≥xima c√©lula para aprender os par√¢metros $w$.\n",
    "- Ap√≥s treinar nossos par√¢metros, usaremos eles para plotar a fronteira de decis√£o.\n",
    "\n",
    "\n",
    "**Observa√ß√£o**\n",
    "\n",
    "O bloco de c√≥digo abaixo leva um tempo consider√°vel para ser executado, especialmente com uma vers√£o n√£o vetorizada. Voc√™ pode reduzir as itera√ß√µes para testar sua implementa√ß√£o e iterar mais r√°pido. Se tiver tempo depois, execute por 100.000 itera√ß√µes para ver resultados melhores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1])-0.5\n",
    "initial_b = 1.\n",
    "\n",
    "# Set regularization parameter lambda_ (you can try varying this)\n",
    "lambda_ = 0.01    \n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_mapped, y_train, initial_w, initial_b, \n",
    "                                    compute_cost_reg, compute_gradient_reg, \n",
    "                                    alpha, iterations, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>Sa√≠da Esperada: Custo < 0.5  (Clique para detalhes)</b>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "# Using the following settings\n",
    "#np.random.seed(1)\n",
    "#initial_w = np.random.rand(X_mapped.shape[1])-0.5\n",
    "#initial_b = 1.\n",
    "#lambda_ = 0.01;                                          \n",
    "#iterations = 10000\n",
    "#alpha = 0.01\n",
    "Iteration    0: Cost     0.72   \n",
    "Iteration 1000: Cost     0.59   \n",
    "Iteration 2000: Cost     0.56   \n",
    "Iteration 3000: Cost     0.53   \n",
    "Iteration 4000: Cost     0.51   \n",
    "Iteration 5000: Cost     0.50   \n",
    "Iteration 6000: Cost     0.48   \n",
    "Iteration 7000: Cost     0.47   \n",
    "Iteration 8000: Cost     0.46   \n",
    "Iteration 9000: Cost     0.45   \n",
    "Iteration 9999: Cost     0.45       \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.7\"></a>\n",
    "### 3.7 Plotagem da Fronteira de Decis√£o\n",
    "\n",
    "Para ajudar voc√™ a visualizar o modelo aprendido por este classificador, usaremos nossa fun√ß√£o `plot_decision_boundary`, que plota a fronteira de decis√£o (n√£o-linear) que separa os exemplos positivos e negativos.\n",
    "\n",
    "- Dentro da fun√ß√£o, plotamos a fronteira de decis√£o n√£o-linear calculando as previs√µes do classificador em uma grade uniformemente espa√ßada e, em seguida, desenhamos um gr√°fico de contorno onde as previs√µes mudam de $y = 0$ para $y = 1$.\n",
    "\n",
    "- Ap√≥s aprender os par√¢metros $w$ e $b$, o pr√≥ximo passo √© plotar uma fronteira de decis√£o semelhante √† Figura 4.\n",
    "\n",
    "<img src=\"images/figure 4.png\" width=\"450\" height=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plotar a fronteira de decis√£o\n",
    "plot_decision_boundary(w, b, X_mapped, y_train)\n",
    "\n",
    "# Adicionar r√≥tulos\n",
    "plt.ylabel('Resultado do Teste 2') \n",
    "plt.xlabel('Resultado do Teste 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8\"></a>\n",
    "### 3.8 Avalia√ß√£o do modelo de Regress√£o Log√≠stica Regularizada\n",
    "\n",
    "Voc√™ usar√° a fun√ß√£o `predict` que voc√™ implementou acima para calcular a acur√°cia do modelo de regress√£o log√≠stica regularizada no conjunto de treinamento (training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular a precis√£o no conjunto de treinamento\n",
    "p = predict(X_mapped, w, b)\n",
    "\n",
    "print('Precis√£o do Treinamento: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Precis√£o do Treinamento<b></td>\n",
    "    <td> ~80% </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
